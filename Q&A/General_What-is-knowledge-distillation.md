# What is knowledge distillation?

Knowledge distillation is a technique used to compress a large, pre-trained
machine learning model into a smaller one that can be more easily deployed in a
mobile device or embedded system. This is done by training the smaller model to
mimic the behavior of the larger one using the outputs of the larger model as
targets. By doing this, the smaller model is able to achieve performance that is
similar to the larger model, but with a significantly smaller size and faster
inference speed.